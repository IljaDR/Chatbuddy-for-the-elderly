# BERT + concat
## BERT model (NLP 1)
In the **Alexa Classifier** folder execute GoS_NLP_Emotion_Classifier_BERT_Best.ipynb on the same data set that can be found in Chatbuddy-for-the-elderly/NLP/LSTM_CNN/datasets/ (Lukas Garbas data set).
The model was created on Google Colab, when executed in this environment no additional dependencies need to be installed.
For integration of the trained BERT model in the project a Python script was created from the trained BERT model. The script can be found here: nlp1_goldfish.py. Dependencies need to be installed through pip install. The script allows to input a written text and will output emotions as derived through the BERT model.
Execution of the BERT model will also generate the outputs from last hidden layer for later use in the ensemble model. The results of this output can be found in alexa_output_all.csv.
nlp1_goldfish_batch.py contains the script that will batch process a csv file and produce the last hidden layer outputs of the trained BERT model producing (alexa_output_all_premerge.csv) that will serve as inputs for the merger script that will provide the input data for the ensemble model. The dependencies need to be installed through pip install.

## LSTM-CNN model (NLP2)
The folder **Daniela Classifier** contains the required information to create the outputs from the LSTM -CNN model (Original NLP2 folder). For instructions on how to implement this model please refer to the LSTM_CNN folder. The outputs from the last hidden layer as generated by this model for the purposes of generating the Ensemble model can be found in daniela_output_all.csv.
The Python script for integration with the project can be found in emofromtweet.py. The script allows to input a written text and will output emotions as derived through the LSTM-CNN model. The script was used to create a batching script for the LSTM-CNN model (emofromtweet_batch.py) that creates the outputs daniela_output_all_premerge.csv that will serve as inputs for the merger script that will provide the input data for the ensemble model.
The script merger.py should now be used to merge the outputs of both model’s predictions, and merge these together with the target emotion labels to create the training data for the ensemble model (merged_a_d_e_all.csv).
## Ensemble model (NLP3)
The folder Stacked Classifier contains the code for the stacked BERT and LSTM – CNN Ensemble model (stacked_ensemble4.ipynb). The script must be executed again on the same data set that can be found in Chatbuddy-for-the-elderly/NLP/LSTM_CNN/datasets/ (Lukas Garbas data set) and takes as input the output generated by merger.py (all required scripts and folders repeated one more time in the folder Merger).

## Citations
The BERT model is based on BERT Fine-Tuning Sentence Classification v2.ipynb
https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=jw5K2A5Ko1RF

The ensemble model code is adapted from a multilabel classifier for the Kaggle Wine challenge
https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab

The data set used throughout the NLP exercise is the Lukas Garbas data set
https://github.com/lukasgarbas/nlp-text-emotion
